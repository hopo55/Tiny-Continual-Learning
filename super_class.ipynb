{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cal-05/anaconda3/envs/hspark/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import learners\n",
    "import dataloaders\n",
    "from dataloaders.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CIFAR100'\n",
    "\n",
    "# prepare dataloader\n",
    "if dataset == 'CIFAR10':\n",
    "    Dataset = dataloaders.iCIFAR10\n",
    "    num_classes = 10\n",
    "elif dataset == 'CIFAR100':\n",
    "    Dataset = dataloaders.iCIFAR100\n",
    "    num_classes = 100\n",
    "elif dataset == 'TinyIMNET':\n",
    "    Dataset = dataloaders.iTinyIMNET\n",
    "    num_classes = 200\n",
    "else:\n",
    "    Dataset = dataloaders.H5Dataset\n",
    "    num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tasks\n",
    "rand_split = True\n",
    "class_order = np.arange(num_classes).tolist()\n",
    "class_order_logits = np.arange(num_classes).tolist()\n",
    "if seed > 0 and rand_split:\n",
    "    random.shuffle(class_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "tasks_logits = []\n",
    "p = 0\n",
    "first_split_size = 5\n",
    "other_split_size = 5\n",
    "\n",
    "while p < num_classes:\n",
    "    inc = other_split_size if p > 0 else first_split_size\n",
    "    tasks.append(class_order[p:p+inc])\n",
    "    tasks_logits.append(class_order_logits[p:p+inc])\n",
    "    p += inc\n",
    "num_tasks = len(tasks)\n",
    "task_names = [str(i+1) for i in range(num_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # Append transform image and buffer image\n",
    "ky = 1 # Not append transform for memory buffer\n",
    "\n",
    "# datasets and dataloaders\n",
    "dataroot = 'data'\n",
    "labeled_samples = 10000 # image per task of CIFAR dataset \n",
    "unlabeled_task_samples = -1\n",
    "l_dist = 'super' # if l_dist is super, then resample task\n",
    "ul_dist = None\n",
    "validation = False\n",
    "repeat = 1\n",
    "\n",
    "train_aug = True\n",
    "train_transform = dataloaders.utils.get_transform(dataset=dataset, phase='train', aug=train_aug)\n",
    "train_transformb = dataloaders.utils.get_transform(dataset=dataset, phase='train', aug=train_aug, hard_aug=True)\n",
    "test_transform  = dataloaders.utils.get_transform(dataset=dataset, phase='test', aug=train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(dataroot, dataset, labeled_samples, unlabeled_task_samples, train=True, lab = True,\n",
    "                        download=True, transform=TransformK(train_transform, train_transform, ky), l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)\n",
    "train_dataset_ul = Dataset(dataroot, dataset, labeled_samples, unlabeled_task_samples, train=True, lab = False,\n",
    "                        download=True, transform=TransformK(train_transform, train_transformb, k), l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)\n",
    "test_dataset  = Dataset(dataroot, dataset, train=False,\n",
    "                        download=False, transform=test_transform, l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab :  True\n",
      "l_dist :  super\n",
      "ul_dist :  super\n",
      "train :  True\n",
      "course_targets :  (50000,)\n",
      "targets :  (50000,)\n",
      "num_classes :  100\n",
      "valid_ul :  (20,)\n",
      "task :  (20, 5)\n",
      "num_sample_ul :  -1\n",
      "archive :  (20, 2, 500)\n",
      "coreset :  (2, 0)\n",
      "coreset :  (array([], dtype=uint8), array([], dtype=int64))\n",
      "class_mapping :  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24330/1508474175.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(\"valid_ul : \",  np.array(train_dataset.valid_ul).shape)\n",
      "/tmp/ipykernel_24330/1508474175.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(\"archive : \", np.array(train_dataset.archive).shape)\n"
     ]
    }
   ],
   "source": [
    "print(\"lab : \", train_dataset.lab)\n",
    "print(\"l_dist : \", train_dataset.l_dist)\n",
    "print(\"ul_dist : \", train_dataset.ul_dist)\n",
    "print(\"train : \", train_dataset.train)\n",
    "print(\"course_targets : \", np.array(train_dataset.course_targets).shape)\n",
    "print(\"targets : \", np.array(train_dataset.targets).shape)\n",
    "print(\"num_classes : \", train_dataset.num_classes)\n",
    "print(\"valid_ul : \",  np.array(train_dataset.valid_ul).shape)\n",
    "print(\"task : \", np.array(train_dataset.tasks).shape)\n",
    "print(\"num_sample_ul : \", train_dataset.num_sample_ul)\n",
    "print(\"archive : \", np.array(train_dataset.archive).shape)\n",
    "print(\"coreset : \", np.array(train_dataset.coreset).shape)\n",
    "print(\"coreset : \", train_dataset.coreset)\n",
    "print(\"class_mapping : \", len(train_dataset.class_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab :  True\n",
      "l_dist :  super\n",
      "ul_dist :  super\n",
      "train :  False\n",
      "course_targets :  (10000,)\n",
      "targets :  (10000,)\n",
      "num_classes :  100\n",
      "valid_ul :  (20,)\n",
      "task :  (20, 5)\n",
      "class_mapping :  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24330/586436881.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(\"valid_ul : \",  np.array(test_dataset.valid_ul).shape)\n"
     ]
    }
   ],
   "source": [
    "print(\"lab : \", test_dataset.lab)\n",
    "print(\"l_dist : \", test_dataset.l_dist)\n",
    "print(\"ul_dist : \", test_dataset.ul_dist)\n",
    "print(\"train : \", test_dataset.train)\n",
    "print(\"course_targets : \", np.array(test_dataset.course_targets).shape)\n",
    "print(\"targets : \", np.array(test_dataset.targets).shape)\n",
    "print(\"num_classes : \", test_dataset.num_classes)\n",
    "print(\"valid_ul : \",  np.array(test_dataset.valid_ul).shape)\n",
    "print(\"task : \", np.array(test_dataset.tasks).shape)\n",
    "print(\"class_mapping : \", len(test_dataset.class_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case tasks reset\n",
    "tasks = train_dataset.tasks\n",
    "\n",
    "# Prepare the Learner (model)\n",
    "workers = 8\n",
    "batch_size = 64\n",
    "ul_batch_size = 128\n",
    "learner_config = {'num_classes': num_classes,\n",
    "                    'lr': 0.1,\n",
    "                    'ul_batch_size': 128,\n",
    "                    'tpr': 0.05, # tpr for ood calibration of class network\n",
    "                    'oodtpr': 0.05, # tpr for ood calibration of ood network\n",
    "                    'momentum': 0.9,\n",
    "                    'weight_decay': 5e-4,\n",
    "                #   'schedule': [120, 160, 180, 200], # schedule and epoch(schedule[-1])\n",
    "                    'schedule': [1, 2, 3, 4],\n",
    "                    'schedule_type': 'decay',\n",
    "                    'model_type': \"resnet\",\n",
    "                    'model_name': \"WideResNet_28_2_cifar\",\n",
    "                    'ood_model_name': 'WideResNet_DC_28_2_cifar',\n",
    "                    'out_dim': 100,\n",
    "                    'optimizer': 'SGD',\n",
    "                    'gpuid': [0],\n",
    "                    'pl_flag': True, # use pseudo-labeled ul data for DM -> ???\n",
    "                    'fm_loss': True, # Use fix-match loss with classifier -> Consistency Regularization / eq.4 -> unsupervised loss\n",
    "                    'weight_aux': 1.0,\n",
    "                    'memory': 400,\n",
    "                    'distill_loss': 'C',\n",
    "                    'co': 1., # out-of-distribution confidence loss ratio\n",
    "                    'FT': True, # finetune distillation -> 이거 필요한가???\n",
    "                    'DW': True, # dataset balancing\n",
    "                    'num_labeled_samples': labeled_samples,\n",
    "                    'num_unlabeled_samples': unlabeled_task_samples,\n",
    "                    'super_flag': l_dist == \"super\",\n",
    "                    'no_unlabeled_data': False\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learners.distillmatch.DistillMatch(learner_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_task :  True\n",
      "oodtpr :  0.05\n",
      "tpr :  0.05\n",
      "num_classes :  100\n",
      "pl_flag :  True\n",
      "prob_threshold_class :  0.0\n",
      "prob_threshold_ood :  0.0\n",
      "fm :  {'thresh': 0.85}\n",
      "model.last.in_features :  128\n",
      "model.last :  Linear(in_features=128, out_features=100, bias=True)\n",
      "reset_optimizer :  True\n",
      "dw :  True\n",
      "dw_thresh :  10.0\n",
      "last_valid_out_dim :  0\n",
      "valid_out_dim :  0\n",
      "memory_size :  400\n",
      "task_count :  0\n",
      "weight_aux :  1.0\n",
      "schedule_type :  decay\n",
      "ft :  True\n",
      "schedule :  [0, 2, 2]\n",
      "distf :  KLDivLoss()\n",
      "tasks :  0\n",
      "past_tasks :  []\n",
      "ood_holdout_ratio :  0.5\n",
      "dc_eps_values :  [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "grad_clip :  1\n",
      "num_deltas :  100\n",
      "num_delta_loop :  10\n"
     ]
    }
   ],
   "source": [
    "print(\"first_task : \", learner.first_task)\n",
    "print(\"oodtpr : \", learner.oodtpr)\n",
    "print(\"tpr : \", learner.tpr)\n",
    "print(\"num_classes : \", learner.num_classes)\n",
    "print(\"pl_flag : \", learner.pl_flag)\n",
    "print(\"prob_threshold_class : \", learner.prob_threshold_class)\n",
    "print(\"prob_threshold_ood : \", learner.prob_threshold_ood)\n",
    "print(\"fm : \", learner.fm)\n",
    "print(\"model.last.in_features : \", learner.model.last.in_features)\n",
    "print(\"model.last : \", learner.model.last)\n",
    "print(\"reset_optimizer : \", learner.reset_optimizer)\n",
    "print(\"dw : \", learner.dw)\n",
    "print(\"dw_thresh : \", learner.dw_thresh)\n",
    "print(\"last_valid_out_dim : \", learner.last_valid_out_dim)\n",
    "print(\"valid_out_dim : \", learner.valid_out_dim)\n",
    "print(\"memory_size : \", learner.memory_size)\n",
    "print(\"task_count : \", learner.task_count)\n",
    "print(\"weight_aux : \", learner.weight_aux)\n",
    "print(\"schedule_type : \", learner.schedule_type)\n",
    "print(\"ft : \", learner.ft)\n",
    "print(\"schedule : \", learner.schedule)\n",
    "print(\"distf : \", learner.distf)\n",
    "print(\"tasks : \", learner.tasks)\n",
    "print(\"past_tasks : \", learner.past_tasks)\n",
    "print(\"ood_holdout_ratio : \", learner.ood_holdout_ratio)\n",
    "print(\"dc_eps_values : \", learner.dc_eps_values)\n",
    "print(\"grad_clip : \", learner.grad_clip)\n",
    "print(\"num_deltas : \", learner.num_deltas)\n",
    "print(\"num_delta_loop : \", learner.num_delta_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== 1 =======================\n",
      "Incremental class: Old valid output dimension: 0\n",
      "Incremental class: New Valid output dimension: 5\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[100. 100. 100. 100. 100.]\n",
      "*************************\n",
      " * Val Acc 20.000, Total time 0.60\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 0.13288981628417967\n",
      "Magnitude: 0.005 loss: 0.13331765747070312\n",
      "Magnitude: 0.001 loss: 0.1322540740966797\n",
      "Magnitude: 0.002 loss: 0.13312376403808593\n",
      "Magnitude: 0.004 loss: 0.1339483642578125\n",
      "Magnitude: 0.08 loss: 0.14960356140136719\n",
      "New Threshold OOD: 0.1405347958 | TPR: 0.0500\n",
      "New Threshold Class: 0.1405347958 | TPR: 0.0500\n",
      "=====================config=====================\n",
      "k :  4\n",
      "data shape :  (1, 80, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  3\n",
      "data shape :  (2, 80, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  2\n",
      "data shape :  (3, 80, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  1\n",
      "data shape :  (4, 80, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  0\n",
      "data shape :  (5, 80, 32, 32, 3)\n",
      "================================================\n",
      "====================== 2 =======================\n",
      "Incremental class: Old valid output dimension: 5\n",
      "Incremental class: New Valid output dimension: 10\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[400. 400. 400. 400. 400. 500. 500. 500. 500. 500.]\n",
      "*************************\n",
      " * Val Acc 20.200, Total time 0.63\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 0.11167552524142794\n",
      "Magnitude: 0.005 loss: 0.11382146199544271\n",
      "Magnitude: 0.001 loss: 0.11094829983181424\n",
      "Magnitude: 0.002 loss: 0.11139380560980902\n",
      "Magnitude: 0.004 loss: 0.11187876383463542\n",
      "Magnitude: 0.08 loss: 0.1428574964735243\n",
      "New Threshold OOD: 0.1473031566 | TPR: 0.0500\n",
      "New Threshold Class: 0.1473031566 | TPR: 0.0500\n",
      "=====================config=====================\n",
      "k :  9\n",
      "data shape :  (1, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  8\n",
      "data shape :  (2, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  7\n",
      "data shape :  (3, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  6\n",
      "data shape :  (4, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  5\n",
      "data shape :  (5, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  4\n",
      "data shape :  (6, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  3\n",
      "data shape :  (7, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  2\n",
      "data shape :  (8, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  1\n",
      "data shape :  (9, 40, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  0\n",
      "data shape :  (10, 40, 32, 32, 3)\n",
      "================================================\n",
      "====================== 3 =======================\n",
      "Incremental class: Old valid output dimension: 10\n",
      "Incremental class: New Valid output dimension: 15\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 500. 500. 500. 500.\n",
      " 500.]\n",
      "*************************\n",
      " * Val Acc 17.067, Total time 0.72\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 4.093632541232639\n",
      "Magnitude: 0.005 loss: 2.3093465169270835\n",
      "Magnitude: 0.001 loss: 2.996872829861111\n",
      "Magnitude: 0.002 loss: 3.708979763454861\n",
      "Magnitude: 0.004 loss: 2.328130425347222\n",
      "Magnitude: 0.08 loss: 10.393409288194444\n",
      "New Threshold OOD: 7.0955088139 | TPR: 0.0500\n",
      "New Threshold Class: 7.0955088139 | TPR: 0.0500\n",
      "=====================config=====================\n",
      "k :  14\n",
      "data shape :  (1, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  13\n",
      "data shape :  (2, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  12\n",
      "data shape :  (3, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  11\n",
      "data shape :  (4, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  10\n",
      "data shape :  (5, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  9\n",
      "data shape :  (6, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  8\n",
      "data shape :  (7, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  7\n",
      "data shape :  (8, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  6\n",
      "data shape :  (9, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  5\n",
      "data shape :  (10, 27, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  4\n",
      "data shape :  (11,)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  3\n",
      "data shape :  (12,)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  2\n",
      "data shape :  (13,)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  1\n",
      "data shape :  (14,)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  0\n",
      "data shape :  (15,)\n",
      "================================================\n",
      "====================== 4 =======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cal-05/heonsung/Tiny-Continual-Learning/dataloaders/loader.py:424: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print('data shape : ', np.array(data).shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental class: Old valid output dimension: 15\n",
      "Incremental class: New Valid output dimension: 20\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[130. 130. 130. 130. 130. 135. 135. 135. 135. 135. 135. 135. 135. 135.\n",
      " 135. 500. 500. 500. 500. 500.]\n",
      "*************************\n",
      " * Val Acc 15.500, Total time 0.80\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 0.9298486328125\n",
      "Magnitude: 0.005 loss: 0.9412433539496528\n",
      "Magnitude: 0.001 loss: 1.013521728515625\n",
      "Magnitude: 0.002 loss: 0.8575259060329861\n",
      "Magnitude: 0.004 loss: 0.8704815673828125\n",
      "Magnitude: 0.08 loss: 1.5289657931857639\n",
      "New Threshold OOD: 3.3981684446 | TPR: 0.0501\n",
      "New Threshold Class: 3.3981684446 | TPR: 0.0501\n",
      "=====================config=====================\n",
      "k :  19\n",
      "data shape :  (1, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  18\n",
      "data shape :  (2, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  17\n",
      "data shape :  (3, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  16\n",
      "data shape :  (4, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  15\n",
      "data shape :  (5, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  14\n",
      "data shape :  (6, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  13\n",
      "data shape :  (7, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  12\n",
      "data shape :  (8, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  11\n",
      "data shape :  (9, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  10\n",
      "data shape :  (10, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  9\n",
      "data shape :  (11, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  8\n",
      "data shape :  (12, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  7\n",
      "data shape :  (13, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  6\n",
      "data shape :  (14, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  5\n",
      "data shape :  (15, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  4\n",
      "data shape :  (16, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  3\n",
      "data shape :  (17, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  2\n",
      "data shape :  (18, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  1\n",
      "data shape :  (19, 20, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  0\n",
      "data shape :  (20, 20, 32, 32, 3)\n",
      "================================================\n",
      "====================== 5 =======================\n",
      "Incremental class: Old valid output dimension: 20\n",
      "Incremental class: New Valid output dimension: 25\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 500. 500. 500. 500. 500.]\n",
      "*************************\n",
      " * Val Acc 12.920, Total time 0.87\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 1.6373396809895833\n",
      "Magnitude: 0.005 loss: 1.2038362630208332\n",
      "Magnitude: 0.001 loss: 0.9832282172309028\n",
      "Magnitude: 0.002 loss: 1.0004571533203126\n",
      "Magnitude: 0.004 loss: 1.1477034505208332\n",
      "Magnitude: 0.08 loss: 2.619143337673611\n",
      "New Threshold OOD: 4.1185286045 | TPR: 0.0500\n",
      "New Threshold Class: 4.1185286045 | TPR: 0.0500\n",
      "=====================config=====================\n",
      "k :  24\n",
      "data shape :  (1, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  23\n",
      "data shape :  (2, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  22\n",
      "data shape :  (3, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  21\n",
      "data shape :  (4, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  20\n",
      "data shape :  (5, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  19\n",
      "data shape :  (6, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  18\n",
      "data shape :  (7, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  17\n",
      "data shape :  (8, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  16\n",
      "data shape :  (9, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  15\n",
      "data shape :  (10, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  14\n",
      "data shape :  (11, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  13\n",
      "data shape :  (12, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  12\n",
      "data shape :  (13, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  11\n",
      "data shape :  (14, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  10\n",
      "data shape :  (15, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  9\n",
      "data shape :  (16, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  8\n",
      "data shape :  (17, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  7\n",
      "data shape :  (18, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  6\n",
      "data shape :  (19, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  5\n",
      "data shape :  (20, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  4\n",
      "data shape :  (21, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  3\n",
      "data shape :  (22, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  2\n",
      "data shape :  (23, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  1\n",
      "data shape :  (24, 16, 32, 32, 3)\n",
      "================================================\n",
      "=====================config=====================\n",
      "k :  0\n",
      "data shape :  (25, 16, 32, 32, 3)\n",
      "================================================\n",
      "====================== 6 =======================\n",
      "Incremental class: Old valid output dimension: 25\n",
      "Incremental class: New Valid output dimension: 30\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[ 80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.\n",
      "  80.  80.  80.  80.  80.  80.  80.  80.  80.  80.  80. 500. 500. 500.\n",
      " 500. 500.]\n",
      "*************************\n",
      " * Val Acc 12.667, Total time 0.96\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 0.8302295600043402\n",
      "Magnitude: 0.005 loss: 0.7735924614800347\n",
      "Magnitude: 0.001 loss: 0.6327851698133681\n",
      "Magnitude: 0.002 loss: 0.6870421685112847\n",
      "Magnitude: 0.004 loss: 0.8585306125217014\n",
      "Magnitude: 0.08 loss: 1.1814873589409722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/dataloaders/loader.py:846\u001b[0m, in \u001b[0;36mSSLDataLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 846\u001b[0m     xu, yul, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munlabeled_iter)\n\u001b[1;32m    847\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown_workers()\n\u001b[0;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb 셀 14\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m model_save_dir \u001b[39m=\u001b[39m log_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/models/repeat-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(seed\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/task-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mtask_names[i]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_save_dir): os\u001b[39m.\u001b[39mmakedirs(model_save_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m learner\u001b[39m.\u001b[39;49mlearn_batch(train_loader, train_dataset, train_dataset_ul, model_save_dir, test_loader)\n",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/learners/distillmatch.py:314\u001b[0m, in \u001b[0;36mDistillMatch.learn_batch\u001b[0;34m(self, train_loader, train_dataset, train_dataset_ul, model_dir, val_loader)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_out_dim_past_past \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_valid_out_dim\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_valid_out_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_out_dim\n\u001b[0;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallibrate_ood_model(train_loader, train_dataset, train_dataset_ul)\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_task \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# Extend memory\u001b[39;00m\n",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/learners/distillmatch.py:567\u001b[0m, in \u001b[0;36mDistillMatch.callibrate_ood_model\u001b[0;34m(self, train_loader, train_dataset, train_dataset_ul)\u001b[0m\n\u001b[1;32m    565\u001b[0m scores_max_all \u001b[39m=\u001b[39m []\n\u001b[1;32m    566\u001b[0m \u001b[39mfor\u001b[39;00m loop \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_delta_loop):  \u001b[39m# num_delta_loop = 10\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mfor\u001b[39;00m i, (xl, y, xul, yul, task)  \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    569\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgpu:\n\u001b[1;32m    570\u001b[0m             xl \u001b[39m=\u001b[39m [xl[k]\u001b[39m.\u001b[39mcuda() \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(xl))]\n",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/dataloaders/loader.py:849\u001b[0m, in \u001b[0;36mSSLDataLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munlabeled_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munlabeled_dset)\n\u001b[0;32m--> 849\u001b[0m     xu, yul, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munlabeled_iter)\n\u001b[1;32m    850\u001b[0m shuffle_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(\u001b[39mlen\u001b[39m(yul), device\u001b[39m=\u001b[39myul\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    851\u001b[0m xu, yul \u001b[39m=\u001b[39m [xu[k][shuffle_idx] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(xu))], yul[shuffle_idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1185\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1189\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1152\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1153\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1154\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/torch/utils/data/dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    978\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    991\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m    992\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    993\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m    994\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m    995\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_table = OrderedDict()\n",
    "acc_table_pt = OrderedDict()\n",
    "run_ood = {}\n",
    "\n",
    "log_dir = \"outputs/CIFAR100-10k/realistic/dm\"\n",
    "save_table = []\n",
    "save_table_pc = -1 * np.ones((num_tasks,num_tasks))\n",
    "pl_table = [[],[],[],[]]\n",
    "temp_dir = log_dir + '/temp'\n",
    "if not os.path.exists(temp_dir): os.makedirs(temp_dir)\n",
    "\n",
    "# Training\n",
    "max_task = -1\n",
    "if max_task > 0:\n",
    "    max_task = min(max_task, len(task_names))\n",
    "else:\n",
    "    max_task = len(task_names)\n",
    "\n",
    "for i in range(max_task):\n",
    "    # set seeds\n",
    "    random.seed(i)\n",
    "    np.random.seed(i)\n",
    "    torch.manual_seed(i)\n",
    "    torch.cuda.manual_seed(i)\n",
    "\n",
    "    train_name = task_names[i]\n",
    "    print('======================', train_name, '=======================')\n",
    "\n",
    "    # load dataset for task\n",
    "    task = tasks_logits[i]\n",
    "    prev = sorted(set([k for task in tasks_logits[:i] for k in task]))\n",
    "\n",
    "    train_dataset.load_dataset(prev, i, train=True)\n",
    "    train_dataset_ul.load_dataset(prev, i, train=True)\n",
    "    out_dim_add = len(task)\n",
    "\n",
    "    # load dataset with memory\n",
    "    train_dataset.append_coreset(only=False)\n",
    "\n",
    "    # load dataloader\n",
    "    train_loader_l = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader_ul = DataLoader(train_dataset_ul, batch_size=ul_batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader_ul_task = DataLoader(train_dataset_ul, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader = dataloaders.SSLDataLoader(train_loader_l, train_loader_ul) # return labeled data, unlabeled data\n",
    "\n",
    "    # add valid class to classifier\n",
    "    learner.add_valid_output_dim(out_dim_add) # return number of classes learned to the current task\n",
    "\n",
    "    # Learn\n",
    "    test_dataset.load_dataset(prev, i, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=workers)\n",
    "\n",
    "    model_save_dir = log_dir + '/models/repeat-'+str(seed+1)+'/task-'+task_names[i]+'/'\n",
    "    if not os.path.exists(model_save_dir): os.makedirs(model_save_dir)\n",
    "\n",
    "    learner.learn_batch(train_loader, train_dataset, train_dataset_ul, model_save_dir, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('hspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7200c97f9580f269913cae0521a31c8c9bc2a022b66e7eee6f7caa3cb908faad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
