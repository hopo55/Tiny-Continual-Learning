{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cal-05/anaconda3/envs/hspark/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import learners\n",
    "import dataloaders\n",
    "from dataloaders.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CIFAR100'\n",
    "\n",
    "# prepare dataloader\n",
    "if dataset == 'CIFAR10':\n",
    "    Dataset = dataloaders.iCIFAR10\n",
    "    num_classes = 10\n",
    "elif dataset == 'CIFAR100':\n",
    "    Dataset = dataloaders.iCIFAR100\n",
    "    num_classes = 100\n",
    "elif dataset == 'TinyIMNET':\n",
    "    Dataset = dataloaders.iTinyIMNET\n",
    "    num_classes = 200\n",
    "else:\n",
    "    Dataset = dataloaders.H5Dataset\n",
    "    num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tasks\n",
    "rand_split = True\n",
    "class_order = np.arange(num_classes).tolist()\n",
    "class_order_logits = np.arange(num_classes).tolist()\n",
    "if seed > 0 and rand_split:\n",
    "    random.shuffle(class_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "tasks_logits = []\n",
    "p = 0\n",
    "first_split_size = 5\n",
    "other_split_size = 5\n",
    "\n",
    "while p < num_classes:\n",
    "    inc = other_split_size if p > 0 else first_split_size\n",
    "    tasks.append(class_order[p:p+inc])\n",
    "    tasks_logits.append(class_order_logits[p:p+inc])\n",
    "    p += inc\n",
    "num_tasks = len(tasks)\n",
    "task_names = [str(i+1) for i in range(num_tasks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # Append transform image and buffer image\n",
    "ky = 1 # Not append transform for memory buffer\n",
    "\n",
    "# datasets and dataloaders\n",
    "dataroot = 'data'\n",
    "labeled_samples = 10000 # image per task of CIFAR dataset \n",
    "unlabeled_task_samples = -1\n",
    "l_dist = 'super' # if l_dist is super, then resample task\n",
    "ul_dist = None\n",
    "validation = False\n",
    "repeat = 1\n",
    "\n",
    "train_aug = True\n",
    "train_transform = dataloaders.utils.get_transform(dataset=dataset, phase='train', aug=train_aug)\n",
    "train_transformb = dataloaders.utils.get_transform(dataset=dataset, phase='train', aug=train_aug, hard_aug=True)\n",
    "test_transform  = dataloaders.utils.get_transform(dataset=dataset, phase='test', aug=train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(dataroot, dataset, labeled_samples, unlabeled_task_samples, train=True, lab = True,\n",
    "                        download=True, transform=TransformK(train_transform, train_transform, ky), l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)\n",
    "train_dataset_ul = Dataset(dataroot, dataset, labeled_samples, unlabeled_task_samples, train=True, lab = False,\n",
    "                        download=True, transform=TransformK(train_transform, train_transformb, k), l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)\n",
    "test_dataset  = Dataset(dataroot, dataset, train=False,\n",
    "                        download=False, transform=test_transform, l_dist=l_dist, ul_dist=ul_dist,\n",
    "                        tasks=tasks, seed=seed, rand_split=rand_split, validation=validation, kfolds=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab :  True\n",
      "l_dist :  super\n",
      "ul_dist :  super\n",
      "train :  True\n",
      "course_targets :  (50000,)\n",
      "targets :  (50000,)\n",
      "num_classes :  100\n",
      "valid_ul :  (20,)\n",
      "task :  (20, 5)\n",
      "num_sample_ul :  -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2420030696.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(\"valid_ul : \",  np.array(train_dataset.valid_ul).shape)\n"
     ]
    }
   ],
   "source": [
    "print(\"lab : \", train_dataset.lab)\n",
    "print(\"l_dist : \", train_dataset.l_dist)\n",
    "print(\"ul_dist : \", train_dataset.ul_dist)\n",
    "print(\"train : \", train_dataset.train)\n",
    "print(\"course_targets : \", np.array(train_dataset.course_targets).shape)\n",
    "print(\"targets : \", np.array(train_dataset.targets).shape)\n",
    "print(\"num_classes : \", train_dataset.num_classes)\n",
    "print(\"valid_ul : \",  np.array(train_dataset.valid_ul).shape)\n",
    "print(\"task : \", np.array(train_dataset.tasks).shape)\n",
    "print(\"num_sample_ul : \", train_dataset.num_sample_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 17, 37, 68, 76, 23, 33, 49, 60, 71], [8, 13, 48, 58, 90, 41, 69, 81, 85, 89], [3, 42, 43, 88, 97, 15, 19, 21, 31, 38, 34, 63, 64, 66, 75, 27, 29, 44, 78, 93, 36, 50, 65, 74, 80], [2, 11, 35, 46, 98], [4, 30, 55, 72, 95, 1, 32, 67, 73, 91], [54, 62, 70, 82, 92, 0, 51, 53, 57, 83, 47, 52, 56, 59, 96], [3, 42, 43, 88, 97, 15, 19, 21, 31, 38, 34, 63, 64, 66, 75, 27, 29, 44, 78, 93, 36, 50, 65, 74, 80], [54, 62, 70, 82, 92, 0, 51, 53, 57, 83, 47, 52, 56, 59, 96], [9, 10, 16, 28, 61, 22, 39, 40, 86, 87, 5, 20, 25, 84, 94], [12, 17, 37, 68, 76, 23, 33, 49, 60, 71], [9, 10, 16, 28, 61, 22, 39, 40, 86, 87, 5, 20, 25, 84, 94], [6, 7, 14, 18, 24, 26, 45, 77, 79, 99], [54, 62, 70, 82, 92, 0, 51, 53, 57, 83, 47, 52, 56, 59, 96], [8, 13, 48, 58, 90, 41, 69, 81, 85, 89], [9, 10, 16, 28, 61, 22, 39, 40, 86, 87, 5, 20, 25, 84, 94], [3, 42, 43, 88, 97, 15, 19, 21, 31, 38, 34, 63, 64, 66, 75, 27, 29, 44, 78, 93, 36, 50, 65, 74, 80], [3, 42, 43, 88, 97, 15, 19, 21, 31, 38, 34, 63, 64, 66, 75, 27, 29, 44, 78, 93, 36, 50, 65, 74, 80], [4, 30, 55, 72, 95, 1, 32, 67, 73, 91], [6, 7, 14, 18, 24, 26, 45, 77, 79, 99], [3, 42, 43, 88, 97, 15, 19, 21, 31, 38, 34, 63, 64, 66, 75, 27, 29, 44, 78, 93, 36, 50, 65, 74, 80]]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.valid_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Learner (model)\n",
    "workers = 8\n",
    "batch_size = 64\n",
    "ul_batch_size = 128\n",
    "learner_config = {'num_classes': num_classes,\n",
    "                    'lr': 0.1,\n",
    "                    'ul_batch_size': 128,\n",
    "                    'tpr': 0.05, # tpr for ood calibration of class network\n",
    "                    'oodtpr': 0.05, # tpr for ood calibration of ood network\n",
    "                    'momentum': 0.9,\n",
    "                    'weight_decay': 5e-4,\n",
    "                #   'schedule': [120, 160, 180, 200], # schedule and epoch(schedule[-1])\n",
    "                    'schedule': [1, 2, 3, 4],\n",
    "                    'schedule_type': 'decay',\n",
    "                    'model_type': \"resnet\",\n",
    "                    'model_name': \"WideResNet_28_2_cifar\",\n",
    "                    'ood_model_name': 'WideResNet_DC_28_2_cifar',\n",
    "                    'out_dim': 100,\n",
    "                    'optimizer': 'SGD',\n",
    "                    'gpuid': [0],\n",
    "                    'pl_flag': True, # use pseudo-labeled ul data for DM -> ???\n",
    "                    'fm_loss': True, # Use fix-match loss with classifier -> Consistency Regularization / eq.4 -> unsupervised loss\n",
    "                    'weight_aux': 1.0,\n",
    "                    'memory': 400,\n",
    "                    'distill_loss': 'C',\n",
    "                    'co': 1., # out-of-distribution confidence loss ratio\n",
    "                    'FT': True, # finetune distillation -> 이거 필요한가???\n",
    "                    'DW': True, # dataset balancing\n",
    "                    'num_labeled_samples': labeled_samples,\n",
    "                    'num_unlabeled_samples': unlabeled_task_samples,\n",
    "                    'super_flag': l_dist == \"super\",\n",
    "                    'no_unlabeled_data': False\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learners.distillmatch.DistillMatch(learner_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_task :  True\n",
      "oodtpr :  0.05\n",
      "tpr :  0.05\n",
      "num_classes :  100\n",
      "pl_flag :  True\n",
      "prob_threshold_class :  0.0\n",
      "prob_threshold_ood :  0.0\n",
      "fm :  {'thresh': 0.85}\n",
      "model.last.in_features :  128\n",
      "model.last :  Linear(in_features=128, out_features=100, bias=True)\n",
      "reset_optimizer :  True\n",
      "dw :  True\n",
      "dw_thresh :  10.0\n",
      "last_valid_out_dim :  0\n",
      "valid_out_dim :  0\n",
      "memory_size :  400\n",
      "task_count :  0\n",
      "weight_aux :  1.0\n",
      "schedule_type :  decay\n",
      "ft :  True\n",
      "schedule :  [0, 2, 2]\n",
      "distf :  KLDivLoss()\n",
      "tasks :  0\n",
      "past_tasks :  []\n",
      "ood_holdout_ratio :  0.5\n",
      "dc_eps_values :  [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "grad_clip :  1\n",
      "num_deltas :  100\n",
      "num_delta_loop :  10\n"
     ]
    }
   ],
   "source": [
    "print(\"first_task : \", learner.first_task)\n",
    "print(\"oodtpr : \", learner.oodtpr)\n",
    "print(\"tpr : \", learner.tpr)\n",
    "print(\"num_classes : \", learner.num_classes)\n",
    "print(\"pl_flag : \", learner.pl_flag)\n",
    "print(\"prob_threshold_class : \", learner.prob_threshold_class)\n",
    "print(\"prob_threshold_ood : \", learner.prob_threshold_ood)\n",
    "print(\"fm : \", learner.fm)\n",
    "print(\"model.last.in_features : \", learner.model.last.in_features)\n",
    "print(\"model.last : \", learner.model.last)\n",
    "print(\"reset_optimizer : \", learner.reset_optimizer)\n",
    "print(\"dw : \", learner.dw)\n",
    "print(\"dw_thresh : \", learner.dw_thresh)\n",
    "print(\"last_valid_out_dim : \", learner.last_valid_out_dim)\n",
    "print(\"valid_out_dim : \", learner.valid_out_dim)\n",
    "print(\"memory_size : \", learner.memory_size)\n",
    "print(\"task_count : \", learner.task_count)\n",
    "print(\"weight_aux : \", learner.weight_aux)\n",
    "print(\"schedule_type : \", learner.schedule_type)\n",
    "print(\"ft : \", learner.ft)\n",
    "print(\"schedule : \", learner.schedule)\n",
    "print(\"distf : \", learner.distf)\n",
    "print(\"tasks : \", learner.tasks)\n",
    "print(\"past_tasks : \", learner.past_tasks)\n",
    "print(\"ood_holdout_ratio : \", learner.ood_holdout_ratio)\n",
    "print(\"dc_eps_values : \", learner.dc_eps_values)\n",
    "print(\"grad_clip : \", learner.grad_clip)\n",
    "print(\"num_deltas : \", learner.num_deltas)\n",
    "print(\"num_delta_loop : \", learner.num_delta_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== 1 =======================\n",
      "Incremental class: Old valid output dimension: 0\n",
      "Incremental class: New Valid output dimension: 5\n",
      "Optimizer is reset!\n",
      "*************************\n",
      "num seen:[100. 100. 100. 100. 100.]\n",
      "*************************\n",
      " * Val Acc 20.000, Total time 0.45\n",
      "=> Load Done\n",
      "Searching the best perturbation magnitude on in-domain data. Magnitude: [0.0025, 0.005, 0.001, 0.002, 0.004, 0.08]\n",
      "Magnitude: 0.0025 loss: 0.13155422973632813\n",
      "Magnitude: 0.005 loss: 0.13203128051757812\n",
      "Magnitude: 0.001 loss: 0.13100776672363282\n",
      "Magnitude: 0.002 loss: 0.13196304321289062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m model_save_dir \u001b[39m=\u001b[39m log_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/models/repeat-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(seed\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/task-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mtask_names[i]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_save_dir): os\u001b[39m.\u001b[39mmakedirs(model_save_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Tiny-Continual-Learning/super_class.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m learner\u001b[39m.\u001b[39;49mlearn_batch(train_loader, train_dataset, train_dataset_ul, model_save_dir, test_loader)\n",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/learners/distillmatch.py:314\u001b[0m, in \u001b[0;36mDistillMatch.learn_batch\u001b[0;34m(self, train_loader, train_dataset, train_dataset_ul, model_dir, val_loader)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_out_dim_past_past \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_valid_out_dim\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_valid_out_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_out_dim\n\u001b[0;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallibrate_ood_model(train_loader, train_dataset, train_dataset_ul)\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_task \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m# Extend memory\u001b[39;00m\n",
      "File \u001b[0;32m~/heonsung/Tiny-Continual-Learning/learners/distillmatch.py:524\u001b[0m, in \u001b[0;36mDistillMatch.callibrate_ood_model\u001b[0;34m(self, train_loader, train_dataset, train_dataset_ul)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mfor\u001b[39;00m i, (xl, y, xul, yul, task)  \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    523\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgpu:\n\u001b[0;32m--> 524\u001b[0m         inputs \u001b[39m=\u001b[39m xl[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda()\n\u001b[1;32m    526\u001b[0m     inputs \u001b[39m=\u001b[39m Variable(inputs, requires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    527\u001b[0m     H, d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mood_model_past\u001b[39m.\u001b[39mood_forward(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_table = OrderedDict()\n",
    "acc_table_pt = OrderedDict()\n",
    "run_ood = {}\n",
    "\n",
    "log_dir = \"outputs/CIFAR100-10k/realistic/dm\"\n",
    "save_table = []\n",
    "save_table_pc = -1 * np.ones((num_tasks,num_tasks))\n",
    "pl_table = [[],[],[],[]]\n",
    "temp_dir = log_dir + '/temp'\n",
    "if not os.path.exists(temp_dir): os.makedirs(temp_dir)\n",
    "\n",
    "# Training\n",
    "max_task = -1\n",
    "if max_task > 0:\n",
    "    max_task = min(max_task, len(task_names))\n",
    "else:\n",
    "    max_task = len(task_names)\n",
    "\n",
    "for i in range(max_task):\n",
    "    # set seeds\n",
    "    random.seed(i)\n",
    "    np.random.seed(i)\n",
    "    torch.manual_seed(i)\n",
    "    torch.cuda.manual_seed(i)\n",
    "\n",
    "    train_name = task_names[i]\n",
    "    print('======================', train_name, '=======================')\n",
    "\n",
    "    # load dataset for task\n",
    "    task = tasks_logits[i]\n",
    "    prev = sorted(set([k for task in tasks_logits[:i] for k in task]))\n",
    "\n",
    "    train_dataset.load_dataset(prev, i, train=True)\n",
    "    train_dataset_ul.load_dataset(prev, i, train=True)\n",
    "    out_dim_add = len(task)\n",
    "\n",
    "    # load dataset with memory\n",
    "    train_dataset.append_coreset(only=False)\n",
    "\n",
    "    # load dataloader\n",
    "    train_loader_l = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader_ul = DataLoader(train_dataset_ul, batch_size=ul_batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader_ul_task = DataLoader(train_dataset_ul, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=int(workers / 2))\n",
    "    train_loader = dataloaders.SSLDataLoader(train_loader_l, train_loader_ul) # return labeled data, unlabeled data\n",
    "\n",
    "    # add valid class to classifier\n",
    "    learner.add_valid_output_dim(out_dim_add) # return number of classes learned to the current task\n",
    "\n",
    "    # Learn\n",
    "    test_dataset.load_dataset(prev, i, train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=workers)\n",
    "\n",
    "    model_save_dir = log_dir + '/models/repeat-'+str(seed+1)+'/task-'+task_names[i]+'/'\n",
    "    if not os.path.exists(model_save_dir): os.makedirs(model_save_dir)\n",
    "\n",
    "    learner.learn_batch(train_loader, train_dataset, train_dataset_ul, model_save_dir, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('hspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7200c97f9580f269913cae0521a31c8c9bc2a022b66e7eee6f7caa3cb908faad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
